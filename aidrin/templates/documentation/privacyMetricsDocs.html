<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Privacy Metrics Guide - AIDRIN</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            line-height: 1.6;
            color: #333;
            background-color: #f8f9fa;
        }

        .container {
            max-width: 1200px;
            margin: 0 auto;
            padding: 20px;
            margin-right: 280px;
            margin-left: 20px;
        }

        .header {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 40px 0;
            text-align: center;
            margin-bottom: 30px;
            margin-right: 280px;
            margin-left: 20px;
        }

        .header h1 {
            font-size: 2.5em;
            margin-bottom: 10px;
        }

        .header p {
            font-size: 1.2em;
            opacity: 0.9;
        }

        .metric-card {
            background: white;
            border-radius: 10px;
            box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1);
            margin-bottom: 30px;
            overflow: hidden;
            transition: transform 0.3s ease, box-shadow 0.3s ease;
        }

        .metric-card:hover {
            transform: translateY(-5px);
            box-shadow: 0 8px 15px rgba(0, 0, 0, 0.15);
        }

        .metric-header {
            background: linear-gradient(135deg, #4facfe 0%, #00f2fe 100%);
            color: white;
            padding: 20px;
            position: relative;
        }

        .metric-header h2 {
            font-size: 1.8em;
            margin-bottom: 10px;
        }

        .metric-header .category {
            background: rgba(255, 255, 255, 0.2);
            padding: 5px 15px;
            border-radius: 20px;
            font-size: 0.9em;
            display: inline-block;
        }

        .metric-content {
            padding: 25px;
        }

        .section {
            margin-bottom: 25px;
        }

        .section h3 {
            color: #2c3e50;
            font-size: 1.3em;
            margin-bottom: 15px;
            border-bottom: 2px solid #3498db;
            padding-bottom: 5px;
        }

        .section ul, .section ol {
            margin: 15px 0;
            padding-left: 25px;
        }

        .section li {
            margin-bottom: 8px;
            line-height: 1.6;
        }

        .section ul li {
            list-style-type: disc;
            color: #555;
        }

        .section ol li {
            list-style-type: decimal;
            color: #555;
        }

        .definition {
            background: #f8f9fa;
            border-left: 4px solid #3498db;
            padding: 15px;
            margin-bottom: 20px;
            border-radius: 0 5px 5px 0;
        }

        .definition ul, .definition ol {
            margin: 10px 0;
            padding-left: 20px;
        }

        .definition li {
            margin-bottom: 6px;
        }

        .example {
            background: #e8f5e8;
            border-left: 4px solid #27ae60;
            padding: 15px;
            margin-bottom: 20px;
            border-radius: 0 5px 5px 0;
        }

        .example ul, .example ol {
            margin: 10px 0;
            padding-left: 20px;
        }

        .example li {
            margin-bottom: 6px;
        }

        .warning {
            background: #fff3cd;
            border-left: 4px solid #ffc107;
            padding: 15px;
            margin-bottom: 20px;
            border-radius: 0 5px 5px 0;
        }

        .warning ul, .warning ol {
            margin: 10px 0;
            padding-left: 20px;
        }

        .warning li {
            margin-bottom: 6px;
        }

        .math-formula {
            background: #f1f2f6;
            border: 1px solid #ddd;
            padding: 15px;
            margin: 15px 0;
            border-radius: 5px;
            font-family: 'Courier New', monospace;
            text-align: center;
        }

        .math-formula + ul,
        .math-formula + ol {
            margin-top: 10px;
        }

        .use-cases {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(250px, 1fr));
            gap: 15px;
            margin-top: 15px;
        }

        .use-case {
            background: #e3f2fd;
            padding: 15px;
            border-radius: 5px;
            border-left: 3px solid #2196f3;
        }

        .use-case h4 {
            color: #1976d2;
            margin-bottom: 8px;
        }

        .parameters {
            background: #fff8e1;
            border: 1px solid #ffb300;
            border-radius: 5px;
            padding: 15px;
            margin-top: 15px;
        }

        .parameters h4 {
            color: #f57c00;
            margin-bottom: 10px;
        }

        .parameter-item {
            display: flex;
            justify-content: space-between;
            align-items: center;
            padding: 8px 0;
            border-bottom: 1px solid #ffcc02;
        }

        .parameter-item:last-child {
            border-bottom: none;
        }

        .parameter-name {
            font-weight: normal;
            color: #333;
        }

        .parameter-desc {
            color: #555;
            text-align: right;
            max-width: 60%;
        }

        .metric-table {
            width: 100%;
            border-collapse: separate;
            border-spacing: 0;
            margin: 15px 0;
            background: white;
            border-radius: 8px;
            overflow: hidden;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        }
        
        .metric-table th,
        .metric-table td {
            padding: 15px 20px;
            text-align: left;
            border-bottom: 1px solid #e0e0e0;
            border-right: 1px solid #e0e0e0;
            vertical-align: top;
        }
        
        .metric-table th:last-child,
        .metric-table td:last-child {
            border-right: none;
        }
        
        .metric-table th {
            background: #f8f9fa;
            font-weight: 600;
            color: #333;
            border-bottom: 2px solid #dee2e6;
        }
        
        .metric-table tr:hover {
            background-color: #f8f9fa;
        }
        
        .metric-table tr:last-child td {
            border-bottom: none;
        }

        .navigation {
            position: fixed;
            top: 20px;
            right: 20px;
            background: white;
            border-radius: 10px;
            box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1);
            padding: 15px;
            max-width: 250px;
            z-index: 1000;
            max-height: calc(100vh - 40px);
            overflow-y: auto;
        }

        .navigation h3 {
            color: #2c3e50;
            margin-bottom: 10px;
            font-size: 1.1em;
        }

        .nav-links {
            list-style: none;
        }

        .nav-links li {
            margin-bottom: 8px;
        }

        .nav-links a {
            color: #3498db;
            text-decoration: none;
            font-size: 0.9em;
            transition: color 0.3s ease;
        }

        .nav-links a:hover {
            color: #2980b9;
            text-decoration: underline;
        }

        .back-button {
            position: fixed;
            bottom: 20px;
            right: 20px;
            background: #3498db;
            color: white;
            border: none;
            padding: 12px 20px;
            border-radius: 25px;
            cursor: pointer;
            font-size: 1em;
            box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1);
            transition: all 0.3s ease;
        }

        .back-button:hover {
            background: #2980b9;
            transform: translateY(-2px);
            box-shadow: 0 6px 8px rgba(0, 0, 0, 0.15);
        }

        @media (max-width: 768px) {
            .container {
                margin-right: 20px;
                margin-left: 20px;
                padding: 15px;
            }
            
            .header {
                margin-right: 20px;
                margin-left: 20px;
                padding: 30px 0;
            }
            
            .header h1 {
                font-size: 2em;
            }
            
            .header p {
                font-size: 1em;
            }
            
            .navigation {
                position: static;
                margin-bottom: 20px;
                max-width: 100%;
                max-height: none;
                overflow-y: visible;
            }
            
            .metric-card {
                margin-bottom: 20px;
            }
            
            .metric-header h2 {
                font-size: 1.5em;
            }
            
            .metric-content {
                padding: 20px;
            }
            
            .section h3 {
                font-size: 1.2em;
            }
            
            .use-cases {
                grid-template-columns: 1fr;
                gap: 10px;
            }
            
            .parameter-item {
                flex-direction: column;
                align-items: flex-start;
                gap: 5px;
            }
            
            .parameter-desc {
                text-align: left;
                max-width: 100%;
            }
            
            .back-button {
                position: static;
                margin-top: 20px;
                width: 100%;
            }
        }

        @media (max-width: 480px) {
            .container {
                padding: 10px;
                margin-right: 10px;
                margin-left: 10px;
            }
            
            .header {
                margin-right: 10px;
                margin-left: 10px;
                padding: 20px 0;
            }
            
            .header h1 {
                font-size: 1.8em;
            }
            
            .header p {
                font-size: 0.9em;
            }
            
            .metric-header {
                padding: 15px;
            }
            
            .metric-header h2 {
                font-size: 1.3em;
            }
            
            .metric-content {
                padding: 15px;
            }
            
            .section h3 {
                font-size: 1.1em;
            }
            
            .math-formula {
                padding: 10px;
                font-size: 0.9em;
                overflow-x: auto;
            }
            
            .definition, .example, .warning {
                padding: 12px;
            }
            
            .use-case {
                padding: 12px;
            }
            
            .parameters {
                padding: 12px;
            }
            
            .parameter-item {
                padding: 6px 0;
            }
        }

        @media (max-width: 1024px) and (min-width: 769px) {
            .container {
                margin-right: 270px;
                margin-left: 20px;
                padding: 20px;
            }
            
            .header {
                margin-right: 270px;
                margin-left: 20px;
            }
            
            .use-cases {
                grid-template-columns: repeat(auto-fit, minmax(200px, 1fr));
            }
        }

        @media (min-width: 1200px) {
            .container {
                max-width: 1400px;
                margin-left: 40px;
            }
            
            .header {
                margin-left: 40px;
            }
            
            .navigation {
                max-width: 280px;
            }
        }
    </style>
</head>
<body>
    <div class="header">
        <h1>Privacy Metrics Guide</h1>
        <p>Comprehensive Guide to Privacy Preservation Techniques and Risk Assessment in AIDRIN</p>
    </div>

    <div class="navigation">
        <h3>Quick Navigation</h3>
        <ul class="nav-links">
            <li><a href="#differential-privacy">Differential Privacy</a></li>
            <li><a href="#single-attribute-risk">Single Attribute Risk</a></li>
            <li><a href="#multiple-attribute-risk">Multiple Attribute Risk</a></li>
            <li><a href="#entropy-risk">Entropy Risk</a></li>
            <li><a href="#k-anonymity">k-Anonymity</a></li>
            <li><a href="#l-diversity">l-Diversity</a></li>
            <li><a href="#t-closeness">t-Closeness</a></li>
        </ul>
    </div>

    <div class="container">
        <!-- Differential Privacy -->
        <div class="metric-card" id="differential-privacy">
            <div class="metric-header">
                <h2>Differential Privacy</h2>
                <span class="category">Noise Addition</span>
            </div>
            <div class="metric-content">
                <div class="section">
                    <h3>Differential Privacy Overview</h3>
                    <div class="definition">
                        Differential privacy represents a rigorous mathematical framework designed to provide provable privacy guarantees through the systematic addition of carefully calibrated noise to computational outputs. This approach ensures that the inclusion or exclusion of any individual record in the dataset has a statistically bounded impact on the results, thereby preventing the identification of specific individuals while maintaining the utility of aggregate statistics.
                    </div>
                </div>

                <div class="section">
                    <h3>Mathematical Foundation</h3>
                    <div class="math-formula">
                        Pr[M(D) ∈ S] ≤ e^ε × Pr[M(D') ∈ S] + δ
                    </div>
                    <p>Where:</p>
                    <ul>
                        <li><strong>M</strong>: The mechanism (algorithm)</li>
                        <li><strong>D, D'</strong>: Neighboring datasets (differing by one record)</li>
                        <li><strong>ε</strong>: Privacy budget (epsilon) - controls privacy vs. utility trade-off</li>
                        <li><strong>δ</strong>: Failure probability (typically very small, e.g., 10^-5)</li>
                    </ul>
                </div>

                <div class="section">
                    <h3>Applications and Use Cases</h3>
                    <div class="use-cases">
                        <div class="use-case">
                            <h4>Statistical Analysis and Reporting</h4>
                            <p>Publication of aggregate statistics from sensitive datasets while maintaining individual privacy</p>
                        </div>
                        <div class="use-case">
                            <h4>Machine Learning and AI</h4>
                            <p>Training predictive models on sensitive data while providing mathematical privacy guarantees</p>
                        </div>
                        <div class="use-case">
                            <h4>Research Data Sharing</h4>
                            <p>Facilitating collaborative research through secure data sharing mechanisms</p>
                        </div>
                        <div class="use-case">
                            <h4>Regulatory Compliance</h4>
                            <p>Meeting stringent privacy requirements under frameworks such as GDPR, HIPAA, or CCPA</p>
                        </div>
                    </div>
                </div>

                <div class="section">
                    <h3>Parameter Selection Guidelines</h3>
                    <div class="warning">
                        <p><strong>Important Disclaimer:</strong> The following parameter guidelines are derived from established research literature and industry best practices. These recommendations serve as general guidance and must be carefully adapted to your specific use case, data sensitivity levels, regulatory requirements, and organizational risk tolerance. These guidelines do not constitute universal standards and may require substantial adjustment for real-world applications.</p>
                    </div>
                    <div class="parameters">
                        <h4>Privacy Budget (ε) Recommendations by Sector</h4>
                        <table class="metric-table">
                            <tr>
                                <th>ε Range</th>
                                <th>Privacy Level</th>
                                <th>Sector & Applications</th>
                            </tr>
                            <tr>
                                <td>ε ≤ 0.1</td>
                                <td>Very High Privacy</td>
                                <td>Healthcare: Medical records, clinical trials, patient data, pharmaceutical research, genetic data, mental health records</td>
                            </tr>
                            <tr>
                                <td>0.1 < ε ≤ 0.5</td>
                                <td>High Privacy</td>
                                <td>Finance: Banking records, credit scores, financial transactions, insurance data, investment portfolios, tax records</td>
                            </tr>
                            <tr>
                                <td>0.5 < ε ≤ 1.0</td>
                                <td>Moderate-High Privacy</td>
                                <td>Education: Student records, academic performance, enrollment data, disciplinary records, special needs information</td>
                            </tr>
                            <tr>
                                <td>1.0 < ε ≤ 2.0</td>
                                <td>Moderate Privacy</td>
                                <td>Research: Academic studies, survey responses, public datasets, social science research, market research data</td>
                            </tr>
                            <tr>
                                <td>2.0 < ε ≤ 5.0</td>
                                <td>Moderate Privacy</td>
                                <td>General Use: Public datasets, non-sensitive analytics, open data initiatives, government statistics</td>
                            </tr>
                            <tr>
                                <td>ε > 5.0</td>
                                <td>Low Privacy</td>
                                <td>Avoid for sensitive data - provides minimal privacy guarantees and should not be used for personal information</td>
                            </tr>
                        </table>
                    </div>
                </div>
                

                
                <div class="section">
                    <h3>Implementation Details</h3>
                    <div class="definition">
                        <p><strong>Current Implementation:</strong> The AIDRIN system implements differential privacy through Laplace noise addition to numerical features. The implementation:</p>
                        <ul>
                            <li>Adds Laplace noise with scale parameter 1/ε to selected numerical columns</li>
                            <li>Generates comparative visualizations showing original vs. noise-added data distributions</li>
                            <li>Provides statistical comparisons (mean, variance) before and after noise addition</li>
                            <li>Saves the noise-added dataset as a CSV file for further analysis</li>
                        </ul>
                        <p><strong>Note:</strong> This implementation focuses on data perturbation rather than risk score computation. The noise addition provides privacy guarantees while maintaining data utility for analysis purposes.</p>
                    </div>
                </div>
                


                <div class="section">
                    <h3>Optimization Strategies and Mitigation Approaches</h3>
                    <div class="example">
                        <h4>Privacy Budget Optimization Strategies:</h4>
                        <ul>
                            <li><strong>Reduce ε parameter:</strong> Decrease the privacy budget to enhance privacy protection levels</li>
                            <li><strong>Increase noise magnitude:</strong> Utilize larger noise scales within the Laplace mechanism framework</li>
                            <li><strong>Query limitation:</strong> Restrict the number of queries to preserve remaining privacy budget</li>
                            <li><strong>Data aggregation:</strong> Implement grouping strategies for similar records to reduce sensitivity</li>
                        </ul>
                        
                        <h4>Utility Enhancement Approaches:</h4>
                        <ul>
                            <li><strong>Careful ε adjustment:</strong> Incrementally increase privacy budget while maintaining privacy requirements</li>
                            <li><strong>Advanced mechanisms:</strong> Implement sophisticated differential privacy algorithms and techniques</li>
                            <li><strong>Data preprocessing:</strong> Clean and normalize datasets to minimize noise requirements</li>
                            <li><strong>Post-processing techniques:</strong> Apply smoothing algorithms or filtering methods to improve result accuracy</li>
                        </ul>
                    </div>
                </div>

                <div class="section">
                    <h3>Limitations and Applicability Constraints</h3>
                    <div class="warning">
                        <h4>Technical Limitations:</h4>
                        <ul>
                            <li><strong>Utility Degradation:</strong> Systematic noise addition inherently compromises data accuracy and precision</li>
                            <li><strong>Parameter Sensitivity:</strong> Output quality is critically dependent on ε and δ parameter selection</li>
                            <li><strong>Implementation Complexity:</strong> Requires sophisticated algorithm design and careful parameter tuning</li>
                            <li><strong>Composition Overhead:</strong> Privacy budget diminishes progressively with multiple query operations</li>
                            <li><strong>Assumption Dependence:</strong> Effectiveness relies heavily on bounded sensitivity assumptions</li>
                        </ul>
                        
                        <h4>Inappropriate Application Scenarios:</h4>
                        <ul>
                            <li><strong>Small-scale datasets:</strong> Noise magnitude may significantly exceed signal strength</li>
                            <li><strong>High-dimensional data:</strong> Privacy budget may be rapidly depleted</li>
                            <li><strong>Non-numerical queries:</strong> Certain query types derive minimal benefit from noise addition</li>
                            <li><strong>Real-time applications:</strong> Computational overhead may render implementation impractical</li>
                            <li><strong>High ε values (ε > 10):</strong> Privacy guarantees become substantially weakened</li>
                        </ul>
                    </div>
                </div>

                <div class="section">
                    <h3>References and Credits</h3>
                    <div class="definition">
                        <p><strong>Foundational Work:</strong></p>
                        <ul>
                            <li>Dwork, C. (2006). <a href="https://link.springer.com/chapter/10.1007/11787006_1" target="_blank">"Differential Privacy."</a> In Proceedings of the 33rd International Colloquium on Automata, Languages and Programming (ICALP).</li>
                            <li>Dwork, C., McSherry, F., Nissim, K., & Smith, A. (2006). <a href="https://link.springer.com/chapter/10.1007/11681878_14" target="_blank">"Calibrating noise to sensitivity in private data analysis."</a> In Theory of Cryptography Conference (TCC).</li>
                        </ul>
                    </div>
                </div>
            </div>
        </div>

        <!-- Single Attribute Risk Score -->
        <div class="metric-card" id="single-attribute-risk">
            <div class="metric-header">
                <h2>Single Attribute Risk Score</h2>
                <span class="category">Re-identification Risk</span>
            </div>
            <div class="metric-content">
                <div class="section">
                    <h3>What is Single Attribute Risk Score?</h3>
                    <div class="definition">
                        Single attribute risk score measures the probability of re-identifying an individual based on a single attribute or feature. It helps identify which attributes pose the highest privacy risk when considered in isolation, providing a baseline assessment of re-identification vulnerability.
                    </div>
                </div>

                <div class="section">
                    <h3>Risk Calculation</h3>
                    <div class="math-formula">
                        Risk<sub>MM</sub>(A) = 1 - [P<sub>start</sub>(A) × P<sub>obs</sub>(A)]
                    </div>
                    <p>
                        Where:<br>
                        <b>P<sub>start</sub>(A)</b> is the probability of observing the attribute value in the dataset.<br>
                        <b>P<sub>obs</sub>(A)</b> is the probability of not observing the same value for the same individual again.<br>
                        The Markov Model-based risk score quantifies the likelihood that an individual can be re-identified based on a single attribute, considering both the frequency and the transition probabilities of attribute values.
                    </p>
                    <div class="definition">
                        <b>Note:</b> This approach is more robust than simple uniqueness, as it accounts for the probability of observing attribute values and their transitions, following the method described in Vatsalan et al. (2023).
                    </div>
                </div>

                <div class="section">
                    <h3>When to Use Single Attribute Risk Score</h3>
                    <div class="use-cases">
                        <div class="use-case">
                            <h4>Initial Data Assessment</h4>
                            <p>Quick screening of attributes to identify obvious privacy risks</p>
                        </div>
                        <div class="use-case">
                            <h4>Anonymization Planning</h4>
                            <p>Determining which attributes need protection or generalization</p>
                        </div>
                        <div class="use-case">
                            <h4>Compliance Auditing</h4>
                            <p>Checking if individual attributes meet privacy requirements</p>
                        </div>
                        <div class="use-case">
                            <h4>Data Release Decisions</h4>
                            <p>Making informed decisions about which attributes can be safely published</p>
                        </div>
                    </div>
                </div>
                
                <div class="section">
                    <h3>Risk Level Recommendations by Sector</h3>
                    <div class="parameters">
                        <table class="metric-table">
                            <tr>
                                <th>Risk Range</th>
                                <th>Risk Level</th>
                                <th>Sector & Applications</th>
                            </tr>
                            <tr>
                                <td>Risk ≤ 0.01</td>
                                <td>Very Low Risk</td>
                                <td>Healthcare: Medical records, patient identifiers, clinical trial data, pharmaceutical research, genetic information, mental health records</td>
                            </tr>
                            <tr>
                                <td>0.01 < Risk ≤ 0.02</td>
                                <td>Low Risk</td>
                                <td>Finance: Banking records, financial identifiers, credit scores, insurance data, investment portfolios, tax records</td>
                            </tr>
                            <tr>
                                <td>0.02 < Risk ≤ 0.05</td>
                                <td>Low-Moderate Risk</td>
                                <td>Education: Student records, academic identifiers, enrollment data, disciplinary records, special needs information, performance metrics</td>
                            </tr>
                            <tr>
                                <td>0.05 < Risk ≤ 0.1</td>
                                <td>Moderate Risk</td>
                                <td>Research: Survey responses, public datasets, academic studies, social science research, market research data</td>
                            </tr>
                            <tr>
                                <td>0.1 < Risk ≤ 0.2</td>
                                <td>Moderate-High Risk</td>
                                <td>General Use: Non-sensitive analytics, open data initiatives, government statistics, public datasets</td>
                            </tr>
                            <tr>
                                <td>Risk > 0.2</td>
                                <td>High Risk</td>
                                <td>Requires immediate attention and anonymization - poses significant re-identification threat</td>
                            </tr>
                        </table>
                    </div>
                </div>
                

                
                <div class="section">
                    <h3>Implementation Details</h3>
                    <div class="definition">
                        <p><strong>Current Implementation:</strong> The AIDRIN system computes single attribute risk scores using a Markov Model approach:</p>
                        <ul>
                            <li>Calculates risk scores for each individual based on attribute value frequencies</li>
                            <li>Uses the formula: Risk = 1 - [P_start(A) × P_obs(A)]</li>
                            <li>Generates box plots showing risk score distributions across features</li>
                            <li>Provides descriptive statistics (mean, std, min, max, percentiles) for each attribute</li>
                            <li>No predefined risk thresholds are applied - interpretation is based on relative values</li>
                        </ul>
                        <p><strong>Note:</strong> The implementation focuses on relative risk assessment rather than absolute threshold-based classification.</p>
                    </div>
                </div>



                <div class="section">
                    <h3>Remedies and Fixes</h3>
                    <div class="example">
                        <h4>If Values Indicate High Risk:</h4>
                        <ul>
                            <li><strong>Generalization:</strong> Group similar values (e.g., ZIP codes to city/state)</li>
                            <li><strong>Suppression:</strong> Remove high-risk attributes entirely</li>
                            <li><strong>Perturbation:</strong> Add noise or randomize values</li>
                            <li><strong>Aggregation:</strong> Combine with other attributes to reduce uniqueness</li>
                            <li><strong>Sampling:</strong> Reduce dataset size to increase anonymity</li>
                        </ul>
                        
                        <h4>If Values are Acceptable:</h4>
                        <ul>
                            <li><strong>Monitor changes:</strong> Track values over time as data evolves</li>
                            <li><strong>Combine with other metrics:</strong> Use alongside multiple attribute risk assessment</li>
                            <li><strong>Document decisions:</strong> Record rationale for acceptable values</li>
                        </ul>
                    </div>
                </div>

                <div class="section">
                    <h3>Limitations and When It's Not Suitable</h3>
                    <div class="warning">
                        <h4>Limitations:</h4>
                        <ul>
                            <li><strong>Oversimplification:</strong> Doesn't account for combinations of attributes</li>
                            <li><strong>Population assumptions:</strong> Assumes uniform distribution of values</li>
                            <li><strong>Context ignorance:</strong> Doesn't consider external knowledge or datasets</li>
                            <li><strong>Static assessment:</strong> Doesn't account for evolving privacy threats</li>
                            <li><strong>No background knowledge:</strong> Doesn't model attacker capabilities</li>
                        </ul>
                        
                        <h4>When Single Attribute Risk is Meaningless:</h4>
                        <ul>
                            <li><strong>Very large datasets:</strong> Most attributes will have low individual risk</li>
                            <li><strong>Highly correlated attributes:</strong> Risk is better assessed in combination</li>
                            <li><strong>Known quasi-identifiers:</strong> When you already know which attributes are risky</li>
                            <li><strong>Complex re-identification scenarios:</strong> Real attacks use multiple attributes</li>
                            <li><strong>When external data exists:</strong> Risk depends on linkage with other datasets</li>
                        </ul>
                    </div>
                </div>

                <div class="section">
                    <h3>References and Credits</h3>
                    <div class="definition">
                        <ul>
                            <li>Vatsalan, D., et al. (2023). <a href="https://bera-journals.onlinelibrary.wiley.com/doi/10.1111/bjet.13223" target="_blank">"Privacy risk quantification in education data using Markov model."</a> British Journal of Educational Technology.</li>
                        </ul>
                    </div>
                </div>
            </div>
        </div>

        <!-- Multiple Attribute Risk Score -->
        <div class="metric-card" id="multiple-attribute-risk">
            <div class="metric-header">
                <h2>Multiple Attribute Risk Score</h2>
                <span class="category">Combined Re-identification Risk</span>
            </div>
            <div class="metric-content">
                <div class="section">
                    <h3>What is Multiple Attribute Risk Score?</h3>
                    <div class="definition">
                        Multiple attribute risk score evaluates the combined risk of re-identification when multiple attributes are considered together. This provides a more realistic assessment of privacy risk, as attackers often use multiple pieces of information to identify individuals. It addresses the fundamental limitation of single attribute assessment by modeling real-world attack scenarios.
                    </div>
                </div>

                <div class="section">
                    <h3>Risk Calculation</h3>
                    <div class="math-formula">
                        Risk<sub>MM</sub>(A₁, ..., Aₙ) = 1 - [Π<sub>i</sub> (P<sub>start</sub>(A<sub>i</sub>) × P<sub>obs</sub>(A<sub>i</sub>) × P<sub>trans</sub>(A<sub>i-1</sub>→A<sub>i</sub>))]
                    </div>
                    <p>
                        Where:<br>
                        <b>P<sub>start</sub>(A<sub>i</sub>)</b> is the probability of observing the value of attribute i.<br>
                        <b>P<sub>obs</sub>(A<sub>i</sub>)</b> is the probability of not observing the same value for the same individual again.<br>
                        <b>P<sub>trans</sub>(A<sub>i-1</sub>→A<sub>i</sub>)</b> is the transition probability between consecutive attributes.<br>
                        The Markov Model-based joint risk score quantifies the likelihood of re-identification when multiple attributes are considered together, capturing both frequency and dependencies between attributes.
                    </p>
                    <div class="definition">
                        <b>Note:</b> This approach models real-world attack scenarios more accurately than simple uniqueness, as it considers both the frequency and transitions of attribute values, following Vatsalan et al. (2023).
                    </div>
                </div>

                <div class="section">
                    <h3>When to Use Multiple Attribute Risk Score</h3>
                    <div class="use-cases">
                        <div class="use-case">
                            <h4>Realistic Attack Modeling</h4>
                            <p>Evaluating actual re-identification scenarios attackers might use</p>
                        </div>
                        <div class="use-case">
                            <h4>Comprehensive Privacy Assessment</h4>
                            <p>Understanding the true privacy risk of your dataset</p>
                        </div>
                        <div class="use-case">
                            <h4>Anonymization Strategy Planning</h4>
                            <p>Determining which attribute combinations need protection</p>
                        </div>
                        <div class="use-case">
                            <h4>Risk Prioritization</h4>
                            <p>Identifying the most dangerous attribute combinations to address first</p>
                        </div>
                        <div class="use-case">
                            <h4>Compliance Validation</h4>
                            <p>Ensuring data meets regulatory privacy requirements</p>
                        </div>
                    </div>
                </div>
                
                <div class="section">
                    <h3>Risk Level Recommendations by Sector</h3>
                    <div class="parameters">
                        <table class="metric-table">
                            <tr>
                                <th>Risk Range</th>
                                <th>Risk Level</th>
                                <th>Sector & Applications</th>
                            </tr>
                            <tr>
                                <td>Risk ≤ 0.005</td>
                                <td>Very Low Risk</td>
                                <td>Healthcare: Medical records, patient combinations, clinical trial data, pharmaceutical research, genetic information, mental health records</td>
                            </tr>
                            <tr>
                                <td>0.005 < Risk ≤ 0.01</td>
                                <td>Low Risk</td>
                                <td>Finance: Banking records, financial combinations, credit scores, insurance data, investment portfolios, tax records</td>
                            </tr>
                            <tr>
                                <td>0.01 < Risk ≤ 0.02</td>
                                <td>Low-Moderate Risk</td>
                                <td>Education: Student records, academic combinations, enrollment data, disciplinary records, special needs information, performance metrics</td>
                            </tr>
                            <tr>
                                <td>0.02 < Risk ≤ 0.05</td>
                                <td>Moderate Risk</td>
                                <td>Research: Survey responses, dataset combinations, academic studies, social science research, market research data</td>
                            </tr>
                            <tr>
                                <td>0.05 < Risk ≤ 0.1</td>
                                <td>Moderate-High Risk</td>
                                <td>General Use: Public datasets, non-sensitive analytics, open data initiatives, government statistics</td>
                            </tr>
                            <tr>
                                <td>Risk > 0.1</td>
                                <td>High Risk</td>
                                <td>Requires immediate attention and anonymization - poses significant re-identification threat</td>
                            </tr>
                        </table>
                        <div class="warning">
                            <p><strong>Note:</strong> Multiple attribute risks are typically higher than single attribute risks due to the increased re-identification potential from attribute combinations.</p>
                        </div>
                    </div>
                </div>
                

                
                <div class="section">
                    <h3>Implementation Details</h3>
                    <div class="definition">
                        <p><strong>Current Implementation:</strong> The AIDRIN system computes multiple attribute risk scores using an extended Markov Model approach:</p>
                        <ul>
                            <li>Calculates joint risk scores considering attribute combinations and transitions</li>
                            <li>Uses the formula: Risk = 1 - [Π(P_start(Ai) × P_obs(Ai) × P_trans(Ai-1→Ai))]</li>
                            <li>Generates box plots showing combined risk score distributions</li>
                            <li>Provides descriptive statistics and a normalized dataset risk score</li>
                            <li>Computes Euclidean distance-based normalization for overall dataset risk assessment</li>
                            <li>No predefined risk thresholds are applied - interpretation is based on relative values</li>
                        </ul>
                        <p><strong>Note:</strong> The implementation provides both individual risk scores and an overall dataset risk assessment.</p>
                    </div>
                </div>



                <div class="section">
                    <h3>Remedies and Fixes</h3>
                    <div class="example">
                        <h4>If Combined Values Indicate High Risk:</h4>
                        <ul>
                            <li><strong>Selective Generalization:</strong> Generalize the most identifying attributes in the combination</li>
                            <li><strong>Attribute Suppression:</strong> Remove one or more attributes from the risky combination</li>
                            <li><strong>Value Perturbation:</strong> Add noise to specific attributes in the combination</li>
                            <li><strong>Record Suppression:</strong> Remove records with unique combinations</li>
                            <li><strong>Hierarchical Generalization:</strong> Use different generalization levels for different attributes</li>
                            <li><strong>Microaggregation:</strong> Group similar records to reduce uniqueness</li>
                        </ul>
                        
                        <h4>If Values are Acceptable:</h4>
                        <ul>
                            <li><strong>Monitor combinations:</strong> Track values for different attribute combinations</li>
                            <li><strong>Document rationale:</strong> Record why certain combinations are acceptable</li>
                            <li><strong>Regular reassessment:</strong> Periodically re-evaluate as data evolves</li>
                            <li><strong>Combine with other metrics:</strong> Use alongside k-anonymity, l-diversity, etc.</li>
                        </ul>
                    </div>
                </div>

                <div class="section">
                    <h3>Limitations and When It's Not Suitable</h3>
                    <div class="warning">
                        <h4>Limitations:</h4>
                        <ul>
                            <li><strong>Combinatorial explosion:</strong> Risk increases exponentially with more attributes</li>
                            <li><strong>Computational complexity:</strong> Can be expensive for many attributes</li>
                            <li><strong>Correlation ignorance:</strong> Doesn't account for attribute correlations</li>
                            <li><strong>External data:</strong> Doesn't consider linkage with other datasets</li>
                            <li><strong>Attack sophistication:</strong> Doesn't model advanced attack techniques</li>
                            <li><strong>Population assumptions:</strong> Assumes uniform distribution across combinations</li>
                        </ul>
                        
                        <h4>When Multiple Attribute Risk is Meaningless:</h4>
                        <ul>
                            <li><strong>Too many attributes:</strong> When combination space becomes too large</li>
                            <li><strong>Highly correlated attributes:</strong> When attributes are functionally dependent</li>
                            <li><strong>Known external linkages:</strong> When external data provides stronger identification</li>
                            <li><strong>Very large datasets:</strong> When most combinations are unique anyway</li>
                            <li><strong>Real-time applications:</strong> When computational overhead is prohibitive</li>
                        </ul>
                    </div>
                </div>

                <div class="section">
                    <h3>References and Credits</h3>
                    <div class="definition">
                        <ul>
                            <li>Vatsalan, D., et al. (2023). <a href="https://bera-journals.onlinelibrary.wiley.com/doi/10.1111/bjet.13223" target="_blank">"Privacy risk quantification in education data using Markov model."</a> British Journal of Educational Technology.</li>
                        </ul>
                    </div>
                </div>
            </div>
        </div>

        <!-- Entropy Risk -->
        <div class="metric-card" id="entropy-risk">
            <div class="metric-header">
                <h2>Entropy Risk</h2>
                <span class="category">Information Theory</span>
            </div>
            <div class="metric-content">
                <div class="section">
                    <h3>Definition</h3>
                    <div class="definition">
                        Entropy risk measures the uncertainty in identifying individuals based on the entropy of equivalence classes formed by quasi-identifiers. Higher entropy indicates lower re-identification risk.
                    </div>
                </div>

                <div class="section">
                    <h3>Mathematical Foundation</h3>
                    <div class="math-formula">
                        H(X) = -Σ p(x) × log₂(p(x))
                    </div>
                    <p>Where H(X) is the entropy of random variable X, and p(x) is the probability of value x.</p>
                </div>

                <div class="section">
                    <h3>Key Parameters</h3>
                    <div class="parameters">
                        <h4>Configuration Options</h4>
                        <div class="parameter-item">
                            <span class="parameter-name">Quasi-Identifiers</span>
                            <span class="parameter-desc">Attributes used to form equivalence classes</span>
                        </div>
                    </div>
                </div>
                
                <div class="section">
                    <h3>Entropy Level Recommendations by Sector</h3>
                    <div class="parameters">
                        <table class="metric-table">
                            <tr>
                                <th>Entropy Range</th>
                                <th>Privacy Level</th>
                                <th>Sector & Applications</th>
                            </tr>
                            <tr>
                                <td>Entropy ≥ 3.0</td>
                                <td>Very High Privacy</td>
                                <td>Healthcare: Medical records, patient data, clinical trial information, pharmaceutical research, genetic data, mental health records, diagnostic information</td>
                            </tr>
                            <tr>
                                <td>Entropy ≥ 2.5</td>
                                <td>High Privacy</td>
                                <td>Finance: Banking records, financial data, credit scores, insurance information, investment portfolios, tax records, transaction history</td>
                            </tr>
                            <tr>
                                <td>Entropy ≥ 2.0</td>
                                <td>Moderate-High Privacy</td>
                                <td>Education: Student records, academic data, enrollment information, disciplinary records, special needs data, performance metrics, attendance records</td>
                            </tr>
                            <tr>
                                <td>Entropy ≥ 1.5</td>
                                <td>Moderate Privacy</td>
                                <td>Research: Survey responses, public datasets, academic studies, social science research, market research data, demographic information</td>
                            </tr>
                            <tr>
                                <td>Entropy ≥ 1.0</td>
                                <td>Moderate Privacy</td>
                                <td>General Use: Public datasets, general analytics, open data initiatives, government statistics, non-sensitive information</td>
                            </tr>
                        </table>
                        <div class="warning">
                            <p><strong>Note:</strong> Higher entropy values indicate better privacy protection. Values below 1.0 generally indicate poor privacy protection and require immediate attention.</p>
                        </div>
                    </div>
                </div>

                <div class="section">
                    <h3>Use Cases</h3>
                    <div class="use-cases">
                        <div class="use-case">
                            <h4>Privacy Assessment</h4>
                            <p>Measuring uncertainty in re-identification</p>
                        </div>
                        <div class="use-case">
                            <h4>Data Quality</h4>
                            <p>Balancing privacy with data utility</p>
                        </div>
                        <div class="use-case">
                            <h4>Anonymization Evaluation</h4>
                            <p>Assessing effectiveness of privacy techniques</p>
                        </div>
                    </div>
                </div>


                

                
                <div class="section">
                    <h3>Implementation Details</h3>
                    <div class="definition">
                        <p><strong>Current Implementation:</strong> The AIDRIN system computes entropy risk based on equivalence class distributions:</p>
                        <ul>
                            <li>Forms equivalence classes based on quasi-identifier combinations</li>
                            <li>Calculates entropy using the standard formula: H(X) = -Σ p(x) × log₂(p(x))</li>
                            <li>Measures uncertainty in re-identification based on class size distributions</li>
                            <li>Higher entropy indicates lower re-identification risk</li>
                            <li>No predefined thresholds are applied - interpretation is based on relative entropy values</li>
                        </ul>
                        <p><strong>Note:</strong> The implementation focuses on information-theoretic privacy assessment rather than threshold-based classification.</p>
                    </div>
                </div>

                <div class="section">
                    <h3>Remedies and Fixes</h3>
                    <div class="example">
                        <h4>If Entropy Values Are Too Low:</h4>
                        <ul>
                            <li><strong>Increase generalization:</strong> Broaden quasi-identifier values to create larger equivalence classes</li>
                            <li><strong>Record suppression:</strong> Remove records that contribute to low entropy</li>
                            <li><strong>Attribute suppression:</strong> Remove problematic quasi-identifiers</li>
                            <li><strong>Microaggregation:</strong> Group similar records to increase class sizes</li>
                            <li><strong>Sampling:</strong> Reduce dataset size to improve entropy distribution</li>
                            <li><strong>Hierarchical generalization:</strong> Use different generalization levels for different attributes</li>
                        </ul>
                        
                        <h4>If Data Utility is Too Low:</h4>
                        <ul>
                            <li><strong>Accept lower entropy:</strong> Balance privacy requirements with data utility needs</li>
                            <li><strong>Selective generalization:</strong> Generalize only the most identifying attributes</li>
                            <li><strong>Use advanced algorithms:</strong> Implement more sophisticated entropy-based techniques</li>
                            <li><strong>Post-processing:</strong> Apply techniques to improve data quality after anonymization</li>
                        </ul>
                    </div>
                </div>

                <div class="section">
                    <h3>Limitations and When It's Not Suitable</h3>
                    <div class="warning">
                        <h4>Limitations:</h4>
                        <ul>
                            <li><strong>Information-theoretic focus:</strong> Doesn't directly model re-identification attacks</li>
                            <li><strong>Distribution assumptions:</strong> Assumes uniform distribution within equivalence classes</li>
                            <li><strong>No background knowledge:</strong> Doesn't account for external information</li>
                            <li><strong>Computational complexity:</strong> Can be expensive for large datasets</li>
                            <li><strong>Utility trade-offs:</strong> Higher entropy may require significant data modification</li>
                            <li><strong>External linkage:</strong> Doesn't prevent linkage with other datasets</li>
                        </ul>
                        
                        <h4>When Entropy Risk is Meaningless:</h4>
                        <ul>
                            <li><strong>Very small datasets:</strong> When entropy cannot be meaningfully calculated</li>
                            <li><strong>High-dimensional data:</strong> When computational overhead is prohibitive</li>
                            <li><strong>When specific attacks matter:</strong> Use k-anonymity, l-diversity, or t-closeness instead</li>
                            <li><strong>Real-time applications:</strong> When computational complexity is too high</li>
                            <li><strong>Binary sensitive attributes:</strong> When sensitive values have limited variety</li>
                        </ul>
                    </div>
                </div>

                <div class="section">
                    <h3>References and Credits</h3>
                    <div class="definition">
                        <p><strong>Foundational Work:</strong></p>
                        <ul>
                            <li>Shannon, C. E. (1948). <a href="https://doi.org/10.1002/j.1538-7305.1948.tb01338.x" target="_blank">"A Mathematical Theory of Communication."</a> The Bell System Technical Journal.</li>
                            <li>Agrawal, R., & Srikant, R. (2000). <a href="https://doi.org/10.1145/335191.335388" target="_blank">"Privacy-preserving data mining."</a> In Proceedings of the 2000 ACM SIGMOD international conference on Management of data.</li>
                        </ul>
                    </div>
                </div>
            </div>
        </div>

        <!-- k-Anonymity -->
        <div class="metric-card" id="k-anonymity">
            <div class="metric-header">
                <h2>k-Anonymity</h2>
                <span class="category">Anonymization Technique</span>
            </div>
            <div class="metric-content">
                <div class="section">
                    <h3>What is k-Anonymity?</h3>
                    <div class="definition">
                        k-Anonymity ensures that each individual in a dataset is indistinguishable from at least k-1 other individuals with respect to quasi-identifiers. This provides protection against re-identification attacks by making it impossible to uniquely identify any individual based on their quasi-identifier values.
                    </div>
                </div>

                <div class="section">
                    <h3>Mathematical Definition</h3>
                    <div class="math-formula">
                        ∀ equivalence class E: |E| ≥ k
                    </div>
                    <p>Every equivalence class (group of records with identical quasi-identifier values) must contain at least k records. This means that any individual cannot be distinguished from at least k-1 others.</p>
                </div>

                <div class="section">
                    <h3>When to Use k-Anonymity</h3>
                    <div class="use-cases">
                        <div class="use-case">
                            <h4>Data Publishing</h4>
                            <p>When releasing datasets to the public or third parties</p>
                        </div>
                        <div class="use-case">
                            <h4>Research Data Sharing</h4>
                            <p>Sharing data for academic or commercial research</p>
                        </div>
                        <div class="use-case">
                            <h4>Healthcare Data Release</h4>
                            <p>Publishing medical datasets for public health research</p>
                        </div>
                        <div class="use-case">
                            <h4>Regulatory Compliance</h4>
                            <p>Meeting privacy requirements for data disclosure</p>
                        </div>
                        <div class="use-case">
                            <h4>Open Data Initiatives</h4>
                            <p>Making government or organizational data publicly available</p>
                        </div>
                    </div>
                </div>

                <div class="section">
                    <h3>Parameter Guidelines</h3>
                    <div class="warning">
                        <p><strong>Important Disclaimer:</strong> The following k-value guidelines are based on anonymization literature and common practices. They serve as general recommendations and should be adapted to your specific use case, data sensitivity, regulatory requirements, and risk tolerance.</p>
                    </div>
                    <div class="parameters">
                        <h4>k-Value Recommendations by Sector</h4>
                        <table class="metric-table">
                            <tr>
                                <th>k Range</th>
                                <th>Protection Level</th>
                                <th>Sector & Applications</th>
                            </tr>
                            <tr>
                                <td>k ≥ 20</td>
                                <td>Very High Protection</td>
                                <td>Healthcare: Medical records, patient data, clinical trial information, pharmaceutical research, genetic data, mental health records, diagnostic information</td>
                            </tr>
                            <tr>
                                <td>k ≥ 15</td>
                                <td>High Protection</td>
                                <td>Finance: Banking records, financial data, credit scores, insurance information, investment portfolios, tax records, transaction history</td>
                            </tr>
                            <tr>
                                <td>k ≥ 10</td>
                                <td>Moderate-High Protection</td>
                                <td>Education: Student records, academic data, enrollment information, disciplinary records, special needs data, performance metrics, attendance records</td>
                            </tr>
                            <tr>
                                <td>k ≥ 5</td>
                                <td>Moderate Protection</td>
                                <td>Research: Survey responses, public datasets, academic studies, social science research, market research data, demographic information</td>
                            </tr>
                            <tr>
                                <td>k ≥ 3</td>
                                <td>Minimal Protection</td>
                                <td>General Use: Public datasets, general analytics, open data initiatives, government statistics, low-risk scenarios</td>
                            </tr>
                        </table>
                    </div>
                </div>
                

                
                <div class="section">
                    <h3>Implementation Details</h3>
                    <div class="definition">
                        <p><strong>Current Implementation:</strong> The AIDRIN system computes k-anonymity as follows:</p>
                        <ul>
                            <li>Groups records by quasi-identifier combinations to form equivalence classes</li>
                            <li>Calculates the minimum class size as the k-value</li>
                            <li>Generates histogram showing distribution of equivalence class sizes</li>
                            <li>Provides descriptive statistics (min, max, mean, median) of class sizes</li>
                            <li>No predefined k thresholds are applied - the system reports the actual k-value achieved</li>
                        </ul>
                        <p><strong>Note:</strong> The implementation reports the actual k-anonymity level achieved rather than applying threshold-based classification.</p>
                    </div>
                </div>

                <div class="section">
                    <h3>Remedies and Fixes</h3>
                    <div class="example">
                        <h4>If k-Anonymity Cannot Be Achieved:</h4>
                        <ul>
                            <li><strong>Generalization:</strong> Broaden attribute values (e.g., exact age → age range)</li>
                            <li><strong>Suppression:</strong> Remove records that cannot be anonymized</li>
                            <li><strong>Microaggregation:</strong> Group similar records and replace with averages</li>
                            <li><strong>Attribute Suppression:</strong> Remove problematic quasi-identifiers</li>
                            <li><strong>Sampling:</strong> Reduce dataset size to increase anonymity</li>
                            <li><strong>Hierarchical Generalization:</strong> Use different generalization levels for different attributes</li>
                        </ul>
                        
                        <h4>If Data Utility is Too Low:</h4>
                        <ul>
                            <li><strong>Reduce k:</strong> Lower the anonymity requirement (balance with privacy needs)</li>
                            <li><strong>Selective Generalization:</strong> Generalize only the most identifying attributes</li>
                            <li><strong>Use advanced algorithms:</strong> Implement more sophisticated anonymization techniques</li>
                            <li><strong>Post-processing:</strong> Apply techniques to improve data quality after anonymization</li>
                        </ul>
                    </div>
                </div>

                <div class="section">
                    <h3>Limitations and When It's Not Suitable</h3>
                    <div class="warning">
                        <h4>Limitations:</h4>
                        <ul>
                            <li><strong>Homogeneity attacks:</strong> All records in a class may have the same sensitive value</li>
                            <li><strong>Background knowledge:</strong> Attackers may have additional information</li>
                            <li><strong>No sensitive attribute protection:</strong> Only protects against re-identification</li>
                            <li><strong>Utility loss:</strong> Generalization reduces data precision</li>
                            <li><strong>Composition attacks:</strong> Multiple releases may compromise privacy</li>
                            <li><strong>External linkage:</strong> Doesn't prevent linkage with other datasets</li>
                        </ul>
                        
                        <h4>When k-Anonymity is Meaningless:</h4>
                        <ul>
                            <li><strong>Very small datasets:</strong> When achieving k > 1 is impossible</li>
                            <li><strong>High-dimensional data:</strong> When quasi-identifiers create too many unique combinations</li>
                            <li><strong>When sensitive attributes matter:</strong> Use l-diversity or t-closeness instead</li>
                            <li><strong>Known external linkages:</strong> When external data can be used for re-identification</li>
                            <li><strong>Real-time applications:</strong> When computational overhead is prohibitive</li>
                        </ul>
                    </div>
                </div>

                <div class="section">
                    <h3>References and Credits</h3>
                    <div class="definition">
                        <p><strong>Foundational Work:</strong></p>
                        <ul>
                            <li>Sweeney, L. (2002). <a href="https://doi.org/10.1142/S0218488502001648" target="_blank">"k-ANONYMITY: A MODEL FOR PROTECTING PRIVACY."</a> International Journal of Uncertainty, Fuzziness and Knowledge-Based Systems.</li>
                        </ul>
                    </div>
                </div>
            </div>
        </div>

        <!-- l-Diversity -->
        <div class="metric-card" id="l-diversity">
            <div class="metric-header">
                <h2>l-Diversity</h2>
                <span class="category">Enhanced Anonymization</span>
            </div>
            <div class="metric-content">
                <div class="section">
                    <h3>What is l-Diversity?</h3>
                    <div class="definition">
                        l-Diversity extends k-anonymity by requiring that each equivalence class contains at least l different values for the sensitive attribute. This protects against homogeneity attacks where all records in a class have the same sensitive value, providing stronger privacy guarantees than k-anonymity alone.
                    </div>
                </div>

                <div class="section">
                    <h3>Mathematical Definition</h3>
                    <div class="math-formula">
                        ∀ equivalence class E: |Unique_Sensitive_Values(E)| ≥ l
                    </div>
                    <p>Each equivalence class must have at least l distinct sensitive attribute values, ensuring diversity in sensitive information within each group.</p>
                </div>

                <div class="section">
                    <h3>When to Use l-Diversity</h3>
                    <div class="use-cases">
                        <div class="use-case">
                            <h4>Enhanced Privacy Protection</h4>
                            <p>When you need stronger privacy than k-anonymity provides</p>
                        </div>
                        <div class="use-case">
                            <h4>Sensitive Attribute Protection</h4>
                            <p>When protecting sensitive attributes is critical</p>
                        </div>
                        <div class="use-case">
                            <h4>Healthcare Data</h4>
                            <p>Protecting medical diagnosis or treatment information</p>
                        </div>
                        <div class="use-case">
                            <h4>Financial Data</h4>
                            <p>Protecting salary, income, or financial status information</p>
                        </div>
                        <div class="use-case">
                            <h4>Research Data</h4>
                            <p>When sensitive outcomes need protection in research datasets</p>
                        </div>
                    </div>
                </div>

                <div class="section">
                    <h3>Parameter Guidelines</h3>
                    <div class="warning">
                        <p><strong>Important Disclaimer:</strong> The following l-value guidelines are based on diversity-based privacy literature and common practices. They serve as general recommendations and should be adapted to your specific use case, data sensitivity, regulatory requirements, and risk tolerance.</p>
                    </div>
                    <div class="parameters">
                        <h4>l-Value Recommendations by Sector</h4>
                        <table class="metric-table">
                            <tr>
                                <th>l Range</th>
                                <th>Diversity Level</th>
                                <th>Sector & Applications</th>
                            </tr>
                            <tr>
                                <td>l ≥ 5</td>
                                <td>Very High Diversity</td>
                                <td>Healthcare: Medical records, patient diagnoses, clinical trial outcomes, pharmaceutical research results, genetic information, mental health assessments</td>
                            </tr>
                            <tr>
                                <td>l ≥ 4</td>
                                <td>High Diversity</td>
                                <td>Finance: Banking records, financial status, credit ratings, insurance claims, investment performance, income levels, debt status</td>
                            </tr>
                            <tr>
                                <td>l ≥ 3</td>
                                <td>Moderate-High Diversity</td>
                                <td>Education: Student records, academic performance, enrollment status, disciplinary actions, special needs classifications, attendance patterns</td>
                            </tr>
                            <tr>
                                <td>l ≥ 2</td>
                                <td>Minimum Diversity</td>
                                <td>Research: Survey responses, public datasets, academic studies, social science research, market research data, demographic information</td>
                            </tr>
                        </table>
                    </div>
                </div>
                

                
                <div class="section">
                    <h3>Implementation Details</h3>
                    <div class="definition">
                        <p><strong>Current Implementation:</strong> The AIDRIN system computes l-diversity as follows:</p>
                        <ul>
                            <li>Groups records by quasi-identifier combinations to form equivalence classes</li>
                            <li>Counts unique sensitive attribute values within each equivalence class</li>
                            <li>Reports the minimum number of distinct sensitive values as the l-value</li>
                            <li>Generates histogram showing distribution of l-diversity across equivalence classes</li>
                            <li>Provides descriptive statistics (min, max, mean, median) of l-diversity values</li>
                            <li>No predefined l thresholds are applied - the system reports the actual l-value achieved</li>
                        </ul>
                        <p><strong>Note:</strong> The implementation reports the actual l-diversity level achieved rather than applying threshold-based classification.</p>
                    </div>
                </div>

                <div class="section">
                    <h3>Remedies and Fixes</h3>
                    <div class="example">
                        <h4>If l-Diversity Cannot Be Achieved:</h4>
                        <ul>
                            <li><strong>Increase generalization:</strong> Broaden quasi-identifier values to create larger equivalence classes</li>
                            <li><strong>Record suppression:</strong> Remove records that cannot achieve l-diversity</li>
                            <li><strong>Sensitive attribute generalization:</strong> Group similar sensitive values</li>
                            <li><strong>Microaggregation:</strong> Group records and replace sensitive values with representatives</li>
                            <li><strong>Attribute suppression:</strong> Remove problematic quasi-identifiers</li>
                            <li><strong>Sampling:</strong> Reduce dataset size to increase diversity</li>
                        </ul>
                        
                        <h4>If Data Utility is Too Low:</h4>
                        <ul>
                            <li><strong>Reduce l:</strong> Lower the diversity requirement (balance with privacy needs)</li>
                            <li><strong>Selective generalization:</strong> Generalize only the most identifying attributes</li>
                            <li><strong>Use advanced algorithms:</strong> Implement more sophisticated l-diversity techniques</li>
                            <li><strong>Post-processing:</strong> Apply techniques to improve data quality after anonymization</li>
                        </ul>
                    </div>
                </div>

                <div class="section">
                    <h3>Limitations and When It's Not Suitable</h3>
                    <div class="warning">
                        <h4>Limitations:</h4>
                        <ul>
                            <li><strong>Skewness attacks:</strong> Sensitive values may still be skewed within classes</li>
                            <li><strong>Background knowledge:</strong> Attackers may have additional information</li>
                            <li><strong>Computational complexity:</strong> Can be expensive for large datasets</li>
                            <li><strong>Utility loss:</strong> May require more aggressive generalization than k-anonymity</li>
                            <li><strong>Not always achievable:</strong> Some datasets cannot achieve l-diversity</li>
                            <li><strong>External linkage:</strong> Doesn't prevent linkage with other datasets</li>
                        </ul>
                        
                        <h4>When l-Diversity is Meaningless:</h4>
                        <ul>
                            <li><strong>Very small datasets:</strong> When achieving l > 1 is impossible</li>
                            <li><strong>Low diversity sensitive attributes:</strong> When sensitive values have limited variety</li>
                            <li><strong>When distribution matters:</strong> Use t-closeness instead for distribution protection</li>
                            <li><strong>High-dimensional data:</strong> When quasi-identifiers create too many unique combinations</li>
                            <li><strong>Real-time applications:</strong> When computational overhead is prohibitive</li>
                        </ul>
                    </div>
                </div>

                <div class="section">
                    <h3>References and Credits</h3>
                    <div class="definition">
                        <p><strong>Foundational Work:</strong></p>
                        <ul>
                            <li>Machanavajjhala, A., Kifer, D., Gehrke, J., & Venkitasubramaniam, M. (2007). <a href="https://dl.acm.org/doi/10.1145/1217299.1217302" target="_blank">"l-diversity: Privacy beyond k-anonymity."</a> ACM Transactions on Knowledge Discovery from Data.</li>
                        </ul>
                    </div>
                </div>
            </div>
        </div>

        <!-- t-Closeness -->
        <div class="metric-card" id="t-closeness">
            <div class="metric-header">
                <h2>t-Closeness</h2>
                <span class="category">Distribution-Based Privacy</span>
            </div>
            <div class="metric-content">
                <div class="section">
                    <h3>What is t-Closeness?</h3>
                    <div class="definition">
                        t-Closeness ensures that the distribution of sensitive attribute values within each equivalence class is close to the overall distribution in the dataset. This prevents skewness attacks where certain sensitive values are overrepresented in specific groups, providing protection against distribution-based privacy breaches.
                    </div>
                </div>

                <div class="section">
                    <h3>Mathematical Definition</h3>
                    <div class="math-formula">
                        ∀ equivalence class E: Distance(P_E, P_global) ≤ t
                    </div>
                    <p>Where P_E is the distribution in equivalence class E, P_global is the global distribution, and t is the closeness threshold. The distance is typically measured using Earth Mover's Distance (EMD) or other distribution distance metrics.</p>
                </div>

                <div class="section">
                    <h3>When to Use t-Closeness</h3>
                    <div class="use-cases">
                        <div class="use-case">
                            <h4>Distribution Protection</h4>
                            <p>When maintaining statistical distribution properties is critical</p>
                        </div>
                        <div class="use-case">
                            <h4>Skewness Attack Prevention</h4>
                            <p>Protecting against attacks that exploit distribution skewness</p>
                        </div>
                        <div class="use-case">
                            <h4>Research Data</h4>
                            <p>When distribution matters for statistical analysis</p>
                        </div>
                        <div class="use-case">
                            <h4>Enhanced Privacy</h4>
                            <p>When you need stronger protection than l-diversity provides</p>
                        </div>
                        <div class="use-case">
                            <h4>Regulatory Compliance</h4>
                            <p>Meeting requirements for distribution-based privacy protection</p>
                        </div>
                    </div>
                </div>

                <div class="section">
                    <h3>Parameter Guidelines</h3>
                    <div class="warning">
                        <p><strong>Important Disclaimer:</strong> The following t-value guidelines are based on distribution-based privacy literature and common practices. They serve as general recommendations and should be adapted to your specific use case, data sensitivity, regulatory requirements, and risk tolerance.</p>
                    </div>
                    <div class="parameters">
                        <h4>t-Value Recommendations by Sector</h4>
                        <table class="metric-table">
                            <tr>
                                <th>t Range</th>
                                <th>Closeness Level</th>
                                <th>Sector & Applications</th>
                            </tr>
                            <tr>
                                <td>t ≤ 0.1</td>
                                <td>Very Close</td>
                                <td>Healthcare: Medical records, patient distributions, clinical trial outcomes, pharmaceutical research results, genetic information, mental health assessments, diagnostic distributions</td>
                            </tr>
                            <tr>
                                <td>0.1 < t ≤ 0.15</td>
                                <td>Close</td>
                                <td>Finance: Banking records, financial distributions, credit ratings, insurance claims, investment performance, income distributions, debt status patterns</td>
                            </tr>
                            <tr>
                                <td>0.15 < t ≤ 0.2</td>
                                <td>Moderate</td>
                                <td>Education: Student records, academic distributions, enrollment patterns, disciplinary actions, special needs classifications, attendance distributions</td>
                            </tr>
                            <tr>
                                <td>0.2 < t ≤ 0.25</td>
                                <td>Moderate</td>
                                <td>Research: Survey responses, dataset distributions, academic studies, social science research, market research data, demographic patterns</td>
                            </tr>
                            <tr>
                                <td>0.25 < t ≤ 0.3</td>
                                <td>Loose</td>
                                <td>General Use: Public datasets, general distributions, open data initiatives, government statistics, non-sensitive information patterns</td>
                            </tr>
                        </table>
                    </div>
                </div>
                

                
                <div class="section">
                    <h3>Implementation Details</h3>
                    <div class="definition">
                        <p><strong>Current Implementation:</strong> The AIDRIN system computes t-closeness as follows:</p>
                        <ul>
                            <li>Groups records by quasi-identifier combinations to form equivalence classes</li>
                            <li>Calculates sensitive attribute distribution within each equivalence class</li>
                            <li>Compares class distributions to the global dataset distribution</li>
                            <li>Uses Total Variation Distance (TVD) to measure distribution differences</li>
                            <li>Reports the maximum distance as the t-value</li>
                            <li>No predefined t thresholds are applied - the system reports the actual t-value achieved</li>
                        </ul>
                        <p><strong>Note:</strong> The implementation reports the actual t-closeness level achieved rather than applying threshold-based classification.</p>
                    </div>
                </div>

                <div class="section">
                    <h3>Remedies and Fixes</h3>
                    <div class="example">
                        <h4>If t-Closeness Cannot Be Achieved:</h4>
                        <ul>
                            <li><strong>Increase generalization:</strong> Broaden quasi-identifier values to create larger equivalence classes</li>
                            <li><strong>Record suppression:</strong> Remove records that cannot achieve t-closeness</li>
                            <li><strong>Sensitive value redistribution:</strong> Redistribute sensitive values across equivalence classes</li>
                            <li><strong>Microaggregation:</strong> Group records and balance sensitive value distributions</li>
                            <li><strong>Attribute suppression:</strong> Remove problematic quasi-identifiers</li>
                            <li><strong>Sampling:</strong> Reduce dataset size to improve distribution matching</li>
                        </ul>
                        
                        <h4>If Data Utility is Too Low:</h4>
                        <ul>
                            <li><strong>Increase t:</strong> Relax the closeness requirement (balance with privacy needs)</li>
                            <li><strong>Selective generalization:</strong> Generalize only the most identifying attributes</li>
                            <li><strong>Use advanced algorithms:</strong> Implement more sophisticated t-closeness techniques</li>
                            <li><strong>Post-processing:</strong> Apply techniques to improve data quality after anonymization</li>
                        </ul>
                    </div>
                </div>

                <div class="section">
                    <h3>Limitations and When It's Not Suitable</h3>
                    <div class="warning">
                        <h4>Limitations:</h4>
                        <ul>
                            <li><strong>Computational complexity:</strong> Can be very expensive for large datasets</li>
                            <li><strong>Distance metric sensitivity:</strong> Results depend on the chosen distance metric</li>
                            <li><strong>Utility loss:</strong> May require significant data modification</li>
                            <li><strong>Not always achievable:</strong> Some datasets cannot achieve t-closeness</li>
                            <li><strong>Background knowledge:</strong> Attackers may have additional information</li>
                            <li><strong>External linkage:</strong> Doesn't prevent linkage with other datasets</li>
                        </ul>
                        
                        <h4>When t-Closeness is Meaningless:</h4>
                        <ul>
                            <li><strong>Very small datasets:</strong> When distributions cannot be meaningfully compared</li>
                            <li><strong>High-dimensional data:</strong> When computational overhead is prohibitive</li>
                            <li><strong>When distribution doesn't matter:</strong> Use k-anonymity or l-diversity instead</li>
                            <li><strong>Real-time applications:</strong> When computational complexity is too high</li>
                            <li><strong>Binary sensitive attributes:</strong> When sensitive values have limited variety</li>
                        </ul>
                    </div>
                </div>

                <div class="section">
                    <h3>Advantages & Limitations</h3>
                    <div class="warning">
                        <h4>Advantages:</h4>
                        <ul>
                            <li>Protects against skewness attacks</li>
                            <li>Maintains statistical properties</li>
                            <li>Strong privacy guarantees</li>
                        </ul>
                        <h4>Limitations:</h4>
                        <ul>
                            <li>Complex to implement</li>
                            <li>May require significant data modification</li>
                            <li>Computationally intensive</li>
                        </ul>
                    </div>
                </div>

                <div class="section">
                    <h3>References and Credits</h3>
                    <div class="definition">
                        <p><strong>Foundational Work:</strong></p>
                        <ul>
                            <li>Li, N., Li, T., & Venkatasubramanian, S. (2007). <a href="https://ieeexplore.ieee.org/document/4221659" target="_blank">"t-closeness: Privacy beyond k-anonymity and l-diversity."</a> In Proceedings of the 23rd International Conference on Data Engineering.</li>
                        </ul>
                    </div>
                </div>
            </div>
        </div>
    </div>

    <button class="back-button" onclick="window.history.back()">← Back to Previous Page</button>

    <script>
        // Smooth scrolling for navigation links
        document.querySelectorAll('.nav-links a').forEach(link => {
            link.addEventListener('click', function(e) {
                e.preventDefault();
                const targetId = this.getAttribute('href').substring(1);
                const targetElement = document.getElementById(targetId);
                if (targetElement) {
                    targetElement.scrollIntoView({
                        behavior: 'smooth',
                        block: 'start'
                    });
                }
            });
        });

        // Add scroll effect to metric cards
        const observerOptions = {
            threshold: 0.1,
            rootMargin: '0px 0px -50px 0px'
        };

        const observer = new IntersectionObserver((entries) => {
            entries.forEach(entry => {
                if (entry.isIntersecting) {
                    entry.target.style.opacity = '1';
                    entry.target.style.transform = 'translateY(0)';
                }
            });
        }, observerOptions);

        document.querySelectorAll('.metric-card').forEach(card => {
            card.style.opacity = '0';
            card.style.transform = 'translateY(20px)';
            card.style.transition = 'opacity 0.6s ease, transform 0.6s ease';
            observer.observe(card);
        });
    </script>
</body>
</html> 